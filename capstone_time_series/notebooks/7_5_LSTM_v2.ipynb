{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast with LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I predict using long short-term memory (LSTM) models.  \n",
    "\n",
    "I start with the San Juan data and forecast with 5 different \"flavors\" of LSTM.  \n",
    "\n",
    "Next, I apply the most-successful approach to the Iquitos data.  \n",
    "\n",
    "Finally, I collect my comments at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate some warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import global libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "import seaborn as sns\n",
    "\n",
    "# # Import converters to allow matplotlib to use dates and eliminte warnings\n",
    "# from pandas.plotting import register_matplotlib_converters\n",
    "# register_matplotlib_converters()\n",
    "\n",
    "# Set some Seaborn defaults\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('paper')\n",
    "sns.set_palette('muted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import keras\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "# from keras.layers import Dropout\n",
    "# from keras.layers import *\n",
    "# from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import ConvLSTM2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import homegrown functions\n",
    "import my_func\n",
    "import importlib  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load whole datasets\n",
    "train_and_test_sj = pd.read_pickle('../sb_cap2_nb-99_data/prep_sj.pickle')\n",
    "train_and_test_iq = pd.read_pickle('../sb_cap2_nb-99_data/prep_iq.pickle')\n",
    "\n",
    "# Load already-split datasets\n",
    "train_sj = pd.read_pickle('../sb_cap2_nb-99_data/prep_train_sj.pickle')\n",
    "test_sj = pd.read_pickle('../sb_cap2_nb-99_data/prep_test_sj.pickle')\n",
    "train_iq = pd.read_pickle('../sb_cap2_nb-99_data/prep_train_iq.pickle')\n",
    "test_iq = pd.read_pickle('../sb_cap2_nb-99_data/prep_test_iq.pickle')\n",
    "\n",
    "# Load already split, log-transformed datasets\n",
    "log_train_sj = pd.read_pickle('../sb_cap2_nb-99_data/prep_log_train_sj.pickle')\n",
    "log_test_sj = pd.read_pickle('../sb_cap2_nb-99_data/prep_log_test_sj.pickle')\n",
    "log_train_iq = pd.read_pickle('../sb_cap2_nb-99_data/prep_log_train_iq.pickle')\n",
    "log_test_iq = pd.read_pickle('../sb_cap2_nb-99_data/prep_log_test_iq.pickle')\n",
    "\n",
    "# Load scores\n",
    "score_df = pd.read_pickle('../sb_cap2_nb-99_data/scores_after_nbk_8.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_prep_raw_series_for_model_scaled(raw_seq, n_steps_in, n_steps_out):\n",
    "    \n",
    "    # Split up the series\n",
    "    X, y = split_sequence(raw_seq, n_steps_in, n_steps_out)\n",
    "    \n",
    "    # Reshape into 3D of [samples, timestamps, features]\n",
    "    X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequence(sequence, n_steps_in, n_steps_out):\n",
    "\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "    \n",
    "    # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        \n",
    "        # check if we are beyond the sequence \n",
    "        if out_end_ix > len(sequence):\n",
    "            break\n",
    "       \n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_the_shape_of_the_array(name, seq):\n",
    "    \n",
    "    print('{} has shape of: {}'.format(name, seq.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_prep_raw_series_for_forcast_scaled(raw_seq, n_steps_in, n_features):\n",
    "    \n",
    "    # Collect last observations of training set as input to forecast\n",
    "    x_input = np.array(raw_seq)\n",
    "    x_input = x_input[(n_steps_in*-1):]\n",
    "    x_input = x_input.reshape((1, n_steps_in, n_features))\n",
    "\n",
    "    return x_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_actual_and_LSTM_forecast_into_single_dataframe(actual, forecast):\n",
    "    \n",
    "    forecast_values = forecast.reshape(forecast.shape[1], forecast.shape[0])\n",
    "    actual_df = actual.copy()\n",
    "    actual_df['forecast'] = forecast_values\n",
    "    actual_df.columns = ['actual', 'forecast']\n",
    "\n",
    "    return actual_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_loss_by_epoch(data, model, city):\n",
    "    \"\"\"Visualize the declining loss acorss my 50 epochs\"\"\"\n",
    "    \n",
    "    g = plt.figure(figsize=(9, 3), dpi=100)\n",
    "    g = plt.xlabel('Epochs')\n",
    "    g = plt.ylabel('Loss')\n",
    "    g = plt.title('Loss by epoch while fitting the {} model for {} data'.format(model, city))\n",
    "    g = plt.xticks(np.arange(0,50,5))\n",
    "    g = plt.plot(range(len(data)), data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sj has a length of 702\n",
      "Test sj has a length of 234\n",
      "That is, I am looking for about a 4 year forecast!\n"
     ]
    }
   ],
   "source": [
    "print('Train sj has a length of {}'.format(len(train_sj)))\n",
    "print('Test sj has a length of {}'.format(len(test_sj)))\n",
    "print('That is, I am looking for about a {} year forecast!'.format(int(len(test_sj)/52)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train iq has a length of 331\n",
      "Test iq has a length of 111\n",
      "That is, I am looking for about a 2 year forecast!\n"
     ]
    }
   ],
   "source": [
    "print('Train iq has a length of {}'.format(len(train_iq)))\n",
    "print('Test iq has a length of {}'.format(len(test_iq)))\n",
    "print('That is, I am looking for about a {} year forecast!'.format(int(len(test_iq)/52)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data - San Jose\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(train_sj)\n",
    "\n",
    "prep_train_sj = scaler.transform(train_sj)\n",
    "prep_train_sj = np.concatenate(prep_train_sj) # Flattens the array\n",
    "prep_train_sj = prep_train_sj.tolist()\n",
    "\n",
    "prep_test_sj = scaler.transform(test_sj)\n",
    "prep_test_sj = np.concatenate(prep_test_sj) # Flattens the array\n",
    "prep_test_sj = prep_test_sj.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data - log tranformed - San Jose\n",
    "scaler_log = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_log.fit(log_train_sj)\n",
    "\n",
    "log_prep_train_sj = scaler_log.transform(log_train_sj)\n",
    "log_prep_train_sj = np.concatenate(log_prep_train_sj) # Flattens the array\n",
    "log_prep_train_sj = log_prep_train_sj.tolist()\n",
    "\n",
    "log_prep_test_sj = scaler_log.transform(log_test_sj)\n",
    "log_prep_test_sj = np.concatenate(log_prep_test_sj) # Flattens the array\n",
    "log_prep_test_sj = log_prep_test_sj.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data - log tranformed - Iquitos\n",
    "scaler_log_iq = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_log_iq.fit(log_train_iq)\n",
    "\n",
    "log_prep_train_iq = scaler_log_iq.transform(log_train_iq)\n",
    "log_prep_train_iq = np.concatenate(log_prep_train_iq) # Flattens the array\n",
    "log_prep_train_iq = log_prep_train_iq.tolist()\n",
    "\n",
    "log_prep_test_iq = scaler_log_iq.transform(log_test_iq)\n",
    "log_prep_test_iq = np.concatenate(log_prep_test_iq) # Flattens the array\n",
    "log_prep_test_iq = log_prep_test_iq.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variation 1 - LSTM - Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up\n",
    "this_run_train = prep_train_sj\n",
    "this_run_test = prep_test_sj\n",
    "this_run_test_original = test_sj\n",
    "this_run_approach = 'LSTM'\n",
    "this_run_details = 'simple'\n",
    "this_run_city = 'san juan'\n",
    "this_run_data_set = 'test'\n",
    "\n",
    "raw_seq = this_run_train\n",
    "\n",
    "this_run_variation = 1\n",
    "this_run_transform = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define other parameters\n",
    "n_steps_out = 234 # Number of timesteps as output.  Aka, forecast length\n",
    "n_steps_in = n_steps_out * 2 # Number of timesteps as input\n",
    "n_features = 1 # Number of features.  Always 1 for this univariate forecast\n",
    "n_units = 100 # Number of notes at each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the time series for model\n",
    "X, y = LSTM_prep_raw_series_for_model_scaled(raw_seq, n_steps_in, n_steps_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify, compile, summarize and fit model\n",
    "\n",
    "# Specify\n",
    "model = Sequential()\n",
    "model.add(LSTM(n_units, activation='relu', input_shape=(n_steps_in, n_features)))\n",
    "model.add(Dense(n_steps_out))\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Summarize\n",
    "model.summary()\n",
    "\n",
    "# Fit\n",
    "model.fit(X, y, epochs=50, verbose=0)\n",
    "\n",
    "# Graph loss\n",
    "graph_loss_by_epoch(model.history.history['loss'], this_run_approach, str.title(this_run_city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the last observations of time series as input to model\n",
    "x_input = LSTM_prep_raw_series_for_forcast_scaled(raw_seq, n_steps_in, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "yhat_scaled = model.predict(x_input, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert scaler\n",
    "yhat = scaler.inverse_transform(yhat_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean \n",
    "clean = combine_actual_and_LSTM_forecast_into_single_dataframe(this_run_test_original, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph\n",
    "my_func.graph_actual_and_forecast_from_test(clean, str.title(this_run_city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "my_func.score(this_run_approach, this_run_variation, \n",
    "              this_run_details, this_run_city, this_run_data_set, this_run_transform, \n",
    "              clean['actual'], clean['forecast'], score_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Score\n",
    "# my_func.score(this_run_approach, this_run_details, this_run_city, this_run_data_set, \n",
    "#               clean['actual'], clean['forecast'], score_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review score\n",
    "score_df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variation 2 - LSTM - Stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up\n",
    "this_run_train = prep_train_sj\n",
    "this_run_test = prep_test_sj\n",
    "this_run_test_original = test_sj\n",
    "this_run_approach = 'LSTM'\n",
    "this_run_details = 'stacked'\n",
    "this_run_city = 'san juan'\n",
    "this_run_data_set = 'test'\n",
    "\n",
    "raw_seq = this_run_train\n",
    "\n",
    "this_run_variation = 2\n",
    "this_run_transform = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define other parameters\n",
    "n_steps_out = 234 # Number of timesteps as output.  Aka, forecast length\n",
    "n_steps_in = n_steps_out * 2 # Number of timesteps as input\n",
    "n_features = 1 # Number of features.  Always 1 for this univariate forecast\n",
    "n_units = 100 # Number of notes at each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the time series for model\n",
    "X, y = LSTM_prep_raw_series_for_model_scaled(raw_seq, n_steps_in, n_steps_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify, compile, summarize and fit model\n",
    "\n",
    "# Specify\n",
    "model = Sequential()\n",
    "model.add(LSTM(n_units, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\n",
    "model.add(LSTM(n_units, activation='relu')) \n",
    "model.add(Dense(n_steps_out))\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Summarize\n",
    "model.summary()\n",
    "\n",
    "# Fit\n",
    "model.fit(X, y, epochs=50, verbose=0)\n",
    "\n",
    "# Graph loss\n",
    "graph_loss_by_epoch(model.history.history['loss'], this_run_approach, str.title(this_run_city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the last observations of time series as input to model\n",
    "x_input = LSTM_prep_raw_series_for_forcast_scaled(raw_seq, n_steps_in, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "yhat_scaled = model.predict(x_input, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert scaler\n",
    "yhat = scaler.inverse_transform(yhat_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean \n",
    "clean = combine_actual_and_LSTM_forecast_into_single_dataframe(this_run_test_original, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph\n",
    "my_func.graph_actual_and_forecast_from_test(clean, str.title(this_run_city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Score\n",
    "# my_func.score(this_run_approach, this_run_details, this_run_city, this_run_data_set, \n",
    "#               clean['actual'], clean['forecast'], score_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "my_func.score(this_run_approach, this_run_variation, \n",
    "              this_run_details, this_run_city, this_run_data_set, this_run_transform, \n",
    "              clean['actual'], clean['forecast'], score_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review score\n",
    "score_df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variation 3 -  LSTM - Stacked - 2.5 times the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up\n",
    "this_run_train = prep_train_sj\n",
    "this_run_test = prep_test_sj\n",
    "this_run_test_original = test_sj\n",
    "this_run_approach = 'LSTM'\n",
    "this_run_details = 'stacked, 2.5x nodes'\n",
    "this_run_city = 'san juan'\n",
    "this_run_data_set = 'test'\n",
    "\n",
    "raw_seq = this_run_train\n",
    "\n",
    "this_run_variation = 3\n",
    "this_run_transform = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define other parameters\n",
    "n_steps_out = 234 # Number of timesteps as output.  Aka, forecast length\n",
    "n_steps_in = n_steps_out * 2 # Number of timesteps as input\n",
    "n_features = 1 # Number of features.  Always 1 for this univariate forecast\n",
    "n_units = 250 # Number of notes at each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the time series for model\n",
    "X, y = LSTM_prep_raw_series_for_model_scaled(raw_seq, n_steps_in, n_steps_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify, compile, summarize and fit model\n",
    "\n",
    "# Specify\n",
    "model = Sequential()\n",
    "model.add(LSTM(n_units, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\n",
    "model.add(LSTM(n_units, activation='relu')) \n",
    "model.add(Dense(n_steps_out))\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Summarize\n",
    "model.summary()\n",
    "\n",
    "# Fit\n",
    "model.fit(X, y, epochs=50, verbose=0)\n",
    "\n",
    "# Graph loss\n",
    "graph_loss_by_epoch(model.history.history['loss'], this_run_approach, str.title(this_run_city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the last observations of time series as input to model\n",
    "x_input = LSTM_prep_raw_series_for_forcast_scaled(raw_seq, n_steps_in, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "yhat_scaled = model.predict(x_input, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert scaler\n",
    "yhat = scaler.inverse_transform(yhat_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean \n",
    "clean = combine_actual_and_LSTM_forecast_into_single_dataframe(this_run_test_original, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph\n",
    "my_func.graph_actual_and_forecast_from_test(clean, str.title(this_run_city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Score\n",
    "# my_func.score(this_run_approach, this_run_details, this_run_city, this_run_data_set, \n",
    "#               clean['actual'], clean['forecast'], score_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "my_func.score(this_run_approach, this_run_variation, \n",
    "              this_run_details, this_run_city, this_run_data_set, this_run_transform, \n",
    "              clean['actual'], clean['forecast'], score_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Review score\n",
    "score_df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variation 4 - LSTM - Bidirectional "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up\n",
    "this_run_train = prep_train_sj\n",
    "this_run_test = prep_test_sj\n",
    "this_run_test_original = test_sj\n",
    "this_run_approach = 'LSTM'\n",
    "this_run_details = 'bidirectional'\n",
    "this_run_city = 'san juan'\n",
    "this_run_data_set = 'test'\n",
    "\n",
    "raw_seq = this_run_train\n",
    "\n",
    "this_run_variation = 4\n",
    "this_run_transform = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define other parameters\n",
    "n_steps_out = 234 # Number of timesteps as output.  Aka, forecast length\n",
    "n_steps_in = n_steps_out * 2 # Number of timesteps as input\n",
    "n_features = 1 # Number of features.  Always 1 for this univariate forecast\n",
    "n_units = 100 # Number of notes at each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the time series for model\n",
    "X, y = LSTM_prep_raw_series_for_model_scaled(raw_seq, n_steps_in, n_steps_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify, compile, summarize and fit model\n",
    "\n",
    "# Specify\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(n_units, activation='relu'), input_shape=(n_steps_in, n_features)))\n",
    "model.add(Dense(n_steps_out))\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Summarize\n",
    "model.summary()\n",
    "\n",
    "# Fit\n",
    "model.fit(X, y, epochs=50, verbose=0)\n",
    "\n",
    "# Graph loss\n",
    "graph_loss_by_epoch(model.history.history['loss'], this_run_approach, str.title(this_run_city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the last observations of time series as input to model\n",
    "x_input = LSTM_prep_raw_series_for_forcast_scaled(raw_seq, n_steps_in, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "yhat_scaled = model.predict(x_input, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert scaler\n",
    "yhat = scaler.inverse_transform(yhat_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean \n",
    "clean = combine_actual_and_LSTM_forecast_into_single_dataframe(this_run_test_original, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph\n",
    "my_func.graph_actual_and_forecast_from_test(clean, str.title(this_run_city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Score\n",
    "# my_func.score(this_run_approach, this_run_details, this_run_city, this_run_data_set, \n",
    "#               clean['actual'], clean['forecast'], score_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "my_func.score(this_run_approach, this_run_variation, \n",
    "              this_run_details, this_run_city, this_run_data_set, this_run_transform, \n",
    "              clean['actual'], clean['forecast'], score_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Review score\n",
    "score_df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variation 5 - LSTM - CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up\n",
    "this_run_train = prep_train_sj\n",
    "this_run_test = prep_test_sj\n",
    "this_run_test_original = test_sj\n",
    "this_run_approach = 'LSTM'\n",
    "this_run_details = 'CNN-LSTM'\n",
    "this_run_city = 'san juan'\n",
    "this_run_data_set = 'test'\n",
    "\n",
    "raw_seq = this_run_train\n",
    "\n",
    "this_run_variation = 5\n",
    "this_run_transform = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define other parameters\n",
    "n_steps_out = 234 # Number of timesteps as output.  Aka, forecast length\n",
    "n_steps_in = n_steps_out * 2 # Number of timesteps as input\n",
    "n_features = 1 # Number of features.  Always 1 for this univariate forecast\n",
    "n_units = 100 # Number of notes at each layer\n",
    "\n",
    "n_seq = 2\n",
    "n_steps2 = int(n_steps_in/n_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the time series for CNN LSTM\n",
    "X, y = split_sequence(raw_seq, n_steps_in, n_steps_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examine_the_shape_of_the_array('Shape of X after first split', X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape into 4D of [samples, subsequences, timesteps, features]\n",
    "X = X.reshape((X.shape[0], n_seq, n_steps2, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify, compile, summarize and fit model\n",
    "\n",
    "# Specify\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv1D(64, 1, activation='relu'), input_shape=(None, n_steps2, n_features)))\n",
    "model.add(TimeDistributed(MaxPooling1D()))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(LSTM(n_units, activation='relu'))\n",
    "model.add(Dense(n_steps_out))\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Summarize\n",
    "model.summary()\n",
    "\n",
    "# Fit\n",
    "model.fit(X, y, epochs=50, verbose=0)\n",
    "\n",
    "# Graph loss\n",
    "graph_loss_by_epoch(model.history.history['loss'], this_run_approach, str.title(this_run_city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the last observations of time series as input to model\n",
    "x_input = np.array(raw_seq)\n",
    "x_input = x_input[(n_steps_in*-1):]\n",
    "x_input = x_input.reshape((1, n_seq, n_steps2, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "yhat_scaled = model.predict(x_input, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert scaler\n",
    "yhat = scaler.inverse_transform(yhat_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean \n",
    "clean = combine_actual_and_LSTM_forecast_into_single_dataframe(this_run_test_original, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph\n",
    "my_func.graph_actual_and_forecast_from_test(clean, str.title(this_run_city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Score\n",
    "# my_func.score(this_run_approach, this_run_details, this_run_city, this_run_data_set, \n",
    "#               clean['actual'], clean['forecast'], score_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "my_func.score(this_run_approach, this_run_variation, \n",
    "              this_run_details, this_run_city, this_run_data_set, this_run_transform, \n",
    "              clean['actual'], clean['forecast'], score_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Review score\n",
    "score_df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variation 6 - ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up\n",
    "this_run_train = prep_train_sj\n",
    "this_run_test = prep_test_sj\n",
    "this_run_test_original = test_sj\n",
    "this_run_approach = 'LSTM'\n",
    "this_run_details = 'ConvLSTM'\n",
    "this_run_city = 'san juan'\n",
    "this_run_data_set = 'test'\n",
    "\n",
    "raw_seq = this_run_train\n",
    "\n",
    "this_run_variation = 6\n",
    "this_run_transform = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define other parameters\n",
    "n_steps_out = 234 # Number of timesteps as output.  Aka, forecast length\n",
    "n_steps_in = n_steps_out * 2 # Number of timesteps as input\n",
    "n_features = 1 # Number of features.  Always 1 for this univariate forecast\n",
    "n_units = 100 # Number of notes at each layer\n",
    "\n",
    "n_seq = 2\n",
    "n_steps2 = int(n_steps_in/n_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the time series for CNN LSTM\n",
    "X, y = split_sequence(raw_seq, n_steps_in, n_steps_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examine_the_shape_of_the_array('Shape of X after first split', X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape into 4D of [samples, subsequences, timesteps, features]\n",
    "X = X.reshape((X.shape[0], n_seq, 1, n_steps2, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify, compile, summarize and fit model\n",
    "\n",
    "# Specify\n",
    "model = Sequential()\n",
    "model.add(ConvLSTM2D(64, (1,2), activation='relu', input_shape=(n_seq, 1, n_steps2, n_features)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(n_steps_out))\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Summarize\n",
    "model.summary()\n",
    "\n",
    "# Fit\n",
    "model.fit(X, y, epochs=50, verbose=0)\n",
    "\n",
    "# Graph loss\n",
    "graph_loss_by_epoch(model.history.history['loss'], this_run_approach, str.title(this_run_city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the last observations of time series as input to model\n",
    "x_input = np.array(raw_seq)\n",
    "x_input = x_input[(n_steps_in*-1):]\n",
    "x_input = x_input.reshape((1, n_seq, 1, n_steps2, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "yhat_scaled = model.predict(x_input, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert scaler\n",
    "yhat = scaler.inverse_transform(yhat_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean \n",
    "clean = combine_actual_and_LSTM_forecast_into_single_dataframe(this_run_test_original, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph\n",
    "my_func.graph_actual_and_forecast_from_test(clean, str.title(this_run_city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "# my_func.score(this_run_approach, this_run_details, this_run_city, this_run_data_set, \n",
    "#               clean['actual'], clean['forecast'], score_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "my_func.score(this_run_approach, this_run_variation, \n",
    "              this_run_details, this_run_city, this_run_data_set, this_run_transform, \n",
    "              clean['actual'], clean['forecast'], score_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review score\n",
    "score_df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variation 7 - LSTM - ConvLSTM - log transformed data - San Juan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up\n",
    "this_run_train = log_prep_train_sj\n",
    "this_run_test = log_prep_test_sj\n",
    "this_run_test_original = test_sj\n",
    "this_run_approach = 'LSTM'\n",
    "this_run_details = 'ConvLSTM'\n",
    "this_run_city = 'san juan'\n",
    "this_run_data_set = 'test'\n",
    "\n",
    "raw_seq = this_run_train\n",
    "\n",
    "this_run_variation = 7\n",
    "this_run_transform = 'log(x+1)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define other parameters\n",
    "n_steps_out = 234 # Number of timesteps as output.  Aka, forecast length\n",
    "n_steps_in = n_steps_out * 2 # Number of timesteps as input\n",
    "n_features = 1 # Number of features.  Always 1 for this univariate forecast\n",
    "n_units = 100 # Number of notes at each layer\n",
    "\n",
    "n_seq = 2\n",
    "n_steps2 = int(n_steps_in/n_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the time series for CNN LSTM\n",
    "X, y = split_sequence(raw_seq, n_steps_in, n_steps_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape into 4D of [samples, subsequences, timesteps, features]\n",
    "X = X.reshape((X.shape[0], n_seq, 1, n_steps2, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify, compile, summarize and fit model\n",
    "\n",
    "# Specify\n",
    "model = Sequential()\n",
    "model.add(ConvLSTM2D(64, (1,2), activation='relu', input_shape=(n_seq, 1, n_steps2, n_features)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(n_steps_out))\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Summarize\n",
    "model.summary()\n",
    "\n",
    "# Fit\n",
    "model.fit(X, y, epochs=50, verbose=0)\n",
    "\n",
    "# Graph loss\n",
    "graph_loss_by_epoch(model.history.history['loss'], this_run_approach, str.title(this_run_city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the last observations of time series as input to model\n",
    "x_input = np.array(raw_seq)\n",
    "x_input = x_input[(n_steps_in*-1):]\n",
    "x_input = x_input.reshape((1, n_seq, 1, n_steps2, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "yhat_log_scaled = model.predict(x_input, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert scaler\n",
    "yhat_log = scaler_log.inverse_transform(yhat_log_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert log\n",
    "yhat = np.expm1(yhat_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean \n",
    "clean = combine_actual_and_LSTM_forecast_into_single_dataframe(this_run_test_original, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph\n",
    "my_func.graph_actual_and_forecast_from_test(clean, str.title(this_run_city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Score\n",
    "# my_func.score(this_run_approach, this_run_details, this_run_city, this_run_data_set, \n",
    "#               clean['actual'], clean['forecast'], score_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "my_func.score(this_run_approach, this_run_variation, \n",
    "              this_run_details, this_run_city, this_run_data_set, this_run_transform, \n",
    "              clean['actual'], clean['forecast'], score_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review score\n",
    "score_df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize - all models from this notebook - San Juan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score_df[(score_df['approach'] == 'LSTM') &  (score_df['city'] == 'san juan')].sort_values('mae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variation 8 - LSTM - ConvLSTM - log transformed data - Iquitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up\n",
    "this_run_train = log_prep_train_iq\n",
    "this_run_test = log_prep_test_iq\n",
    "this_run_test_original = test_iq\n",
    "this_run_approach = 'LSTM'\n",
    "this_run_details = 'ConvLSTM'\n",
    "this_run_city = 'iquitos'\n",
    "this_run_data_set = 'test'\n",
    "\n",
    "raw_seq = this_run_train\n",
    "\n",
    "this_run_variation = 8\n",
    "this_run_transform = 'log(x+1)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define other parameters\n",
    "n_steps_out = 111 # Number of timesteps as output.  Aka, forecast length\n",
    "n_steps_in = int(n_steps_out * 1.5) # Number of timesteps as input\n",
    "n_features = 1 # Number of features.  Always 1 for this univariate forecast\n",
    "n_units = 100 # Number of notes at each layer\n",
    "\n",
    "n_seq = 2\n",
    "n_steps2 = int(n_steps_in/n_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the time series for CNN LSTM\n",
    "X, y = split_sequence(raw_seq, n_steps_in, n_steps_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape into 4D of [samples, subsequences, timesteps, features]\n",
    "X = X.reshape((X.shape[0], n_seq, 1, n_steps2, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify, compile, summarize and fit model\n",
    "\n",
    "# Specify\n",
    "model = Sequential()\n",
    "model.add(ConvLSTM2D(64, (1,2), activation='relu', input_shape=(n_seq, 1, n_steps2, n_features)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(n_steps_out))\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Summarize\n",
    "model.summary()\n",
    "\n",
    "# Fit\n",
    "model.fit(X, y, epochs=50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the last observations of time series as input to model\n",
    "x_input = np.array(raw_seq)\n",
    "x_input = x_input[(n_steps_in*-1):]\n",
    "x_input = x_input.reshape((1, n_seq, 1, n_steps2, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "yhat_log_scaled = model.predict(x_input, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert scaler\n",
    "yhat_log = scaler_log_iq.inverse_transform(yhat_log_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert log\n",
    "yhat = np.expm1(yhat_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean \n",
    "clean = combine_actual_and_LSTM_forecast_into_single_dataframe(this_run_test_original, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph\n",
    "my_func.graph_actual_and_forecast_from_test(clean, str.title(this_run_city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "# my_func.score(this_run_approach, this_run_details, this_run_city, this_run_data_set, \n",
    "#               clean['actual'], clean['forecast'], score_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "my_func.score(this_run_approach, this_run_variation, \n",
    "              this_run_details, this_run_city, this_run_data_set, this_run_transform, \n",
    "              clean['actual'], clean['forecast'], score_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Review score\n",
    "score_df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize - all models - San Juan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = score_df[(score_df['data'] == 'test') & (score_df['city'] == 'san juan')] \n",
    "summary.sort_values(by=['city', 'mae'], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize - all models - Iquitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = score_df[(score_df['data'] == 'test') & (score_df['city'] == 'iquitos')] \n",
    "summary.sort_values(by=['city', 'mae'], ascending=[False, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df.to_pickle('../sb_cap2_nb-99_data/scores_after_nbk_9.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df.to_csv('../sb_cap2_nb-99_data/scores_after_nbk_9.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I focus on long short-term memory models (LSTM).  LSTM are deep-learning models based on recurrent neural network (RNN) architectures.  LSTM networks have been used successfully to forecast time series because LTSM learn temporal dependencies.  LSTM accomplish this by their use of cells, input gates, output gates and forget gates.  This architecture allows the LSTM to read time series one time step at a time and build representations to be used in prediction.\n",
    "\n",
    "There are a variety of LSTM.  In this notebook, I focus on simple LSTM, stacked LSTM, bidirectional LSTM, CNN-LSTM and ConvLSTM.\n",
    "\n",
    "These models were a bit finicky, I found.  I need to:  get the multi-step input sequences in just the right order (i.e., samples of 100 X's and 50 y's), get the data formatting just right (i.e., lists v. arrays v. dataframes), and shape the data into the just right dimensions (ie., from 1D time series into 3D, 4D and 5D arrays).  But this is standard practice, of course.  For me, some of the pre-packaged functions to help with this (i.e., TimeSeriesGenerator) weren't easily adaptable to my multi-step time series problem.\n",
    "\n",
    "**Simple LSTM**\n",
    "\n",
    "* This model has one hidden layer of LSTM units and one output layer.\n",
    "* I use 100 units as the default for all my LSTM models.\n",
    "* My simple LSTM model is the baseline with an MAE of around 25.\n",
    "\n",
    "**Stacked LSTM**\n",
    "\n",
    "* Stacked models use multiple layers of hidden LSTM.\n",
    "* My stacked LSTM model uses such layers.  This model improved my MAE to a bit below 25.\n",
    "* Increasing the number of unit per LSTM node from 100 to 250 did not improve the model's performance. So, I continue to use 100 units in my later models.\n",
    "\n",
    "**Bidirectional LSTM**\n",
    "\n",
    "* In a bidirectional models, the LSTM learns the sequence both backward and forward.\n",
    "* My bidirectional LSTM did worse than my baseline simple LSTM.\n",
    "\n",
    "**CNN-LSTM**\n",
    "\n",
    "* These are hybrid model that first process data through a convolutional neural network (CNN) and then through a simple LSTM.  These added CNN layers are to improve the model's ability to learn key one-dimensional sequences.\n",
    "* My CNN-LSTM outperforms all prior LSTM models with an MAE of around 23.\n",
    "\n",
    "**ConvLSTM**\n",
    "\n",
    "* ConvLSTM are hybrid models, too.  They are much like CNN-LSTM.  However, the CNN-related layer is build into the LSTM layer, rather than being before the LSTM layer.\n",
    "* My initial ConvLSTM has the best performance so far with an MAE of around near 22.\n",
    "* To further improve perofrmance, I model off log-transformed data.  The score improved again with an MAE of around 21.\n",
    "* The ConvLSTM on log-transformed data is the best of my LSTM models.\n",
    "\n",
    "After using the San Juan data to select the model, I run the best-performing model on the Iquitos data.\n",
    "\n",
    "Overall, the LSTM models do not perform very well.  ConvLSTM, the best of the LSTM, performs worse than the best of the Facebook Prophet, exponential smoothing and ARMIA models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
